{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUotKHRULVPD"
   },
   "source": [
    "# Инструменты для работы с языком "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ba5Z02VLVPK"
   },
   "source": [
    "## Задача: классификация твитов по тональности\n",
    "\n",
    "У нас есть датасет из твитов, про каждый указано, как он эмоционально окрашен: положительно или отрицательно. Задача: предсказывать эмоциональную окраску.\n",
    "\n",
    "Скачиваем куски датасета ([источник](http://study.mokoron.com/)): [положительные](https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0), [отрицательные](https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "J5YiZNCPLVPe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DFLtXAZ-LVPq"
   },
   "outputs": [],
   "source": [
    "# считываем данные и заполняем общий датасет\n",
    "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
    "positive['label'] = ['positive'] * len(positive)\n",
    "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
    "negative['label'] = ['negative'] * len(negative)\n",
    "df = positive.append(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZWta7oDgLVP8"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAapBC7VLVQC"
   },
   "source": [
    "## Baseline: классификация необработанных n-грамм\n",
    "\n",
    "### Векторизаторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "M-AvVt8XLVQD"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # можно заменить на любимый классификатор\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSuoVoxcLVQI"
   },
   "source": [
    "Что такое n-граммы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zeNA7732LVQJ"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AJk1B39LVRP"
   },
   "source": [
    "## Стоп-слова и пунктуация\n",
    "\n",
    "*Стоп-слова* -- это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конретном документе, то есть играют роль шума. Поэтому их принято убирать. По той же причине убирают и пунктуацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpWhsTuRLVRP",
    "outputId": "1cd18efe-a3cd-4f56-ec4c-bec8b6ee442e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/vlad/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# у вас здесь, вероятно, выскочит ошибка и надо будет загрузить стоп слова (в тексте ошибки написано, как)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "OdRF7rlyLVRS",
    "outputId": "dd4ce4f0-13d0-4b21-a3a1-b9ecb281894f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OfXiH98XLVRV"
   },
   "outputs": [],
   "source": [
    "noise = stopwords.words('russian') + list(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtiIhHDMLVRY"
   },
   "source": [
    "В векторизаторах за стоп-слова, логичным образом, отвечает аргумент `stop_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aP5qFnilLVSI"
   },
   "source": [
    "## Словарь, закон Ципфа и закон Хипса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1umtd3OLVSI"
   },
   "source": [
    "Закон Ципфа -- эмпирическая закономерность: если все слова корпуса текста упорядочить по убыванию частоты их использования, то частота n-го слова в таком списке окажется приблизительно обратно пропорциональной его порядковому номеру n. Иными словами, частотность слов убывает очень быстро."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "lY0cWJ7eLVSJ"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vIjqSVjpLVSL",
    "outputId": "a75ec748-ab21-4bd0-cd41-5a9fa8362b22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2870536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['first_timee', 'хоть', 'я', 'и', 'школота', 'но', 'поверь', 'у', 'нас', 'то']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [token for tweet in df.text for token in word_tokenize(tweet) if token not in punctuation]\n",
    "print(len(corpus))\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "_oWC7NpkLVSO",
    "outputId": "965b9fbd-6328-4c13-f20f-cd3714d1adb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('не', 69472),\n",
       " ('и', 55166),\n",
       " ('в', 52902),\n",
       " ('я', 52818),\n",
       " ('RT', 38070),\n",
       " ('на', 35759),\n",
       " ('http', 32998),\n",
       " ('что', 31541),\n",
       " ('с', 27217),\n",
       " ('а', 26860)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dict = Counter(corpus)\n",
    "freq_dict_sorted= sorted(freq_dict.items(), key=lambda x: -x[1])\n",
    "list(freq_dict_sorted)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "FrPkce0SLVSQ",
    "outputId": "d2ab5675-433a-480a-90ee-fa5dd9890922"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "first_100_freqs = [freq for word, freq in freq_dict_sorted[:100]]\n",
    "plt.plot(first_100_freqs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_N5V_K-LVSU"
   },
   "source": [
    "Закон Хипса -- обратная сторона закона Ципфа. Он описывает, что чем больше корпус, тем меньше новых слов добавляется с добавлением новых текстов. В какой-то момент корпус насыщается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw0GieJSMU-O"
   },
   "source": [
    "## Задание 1.\n",
    "\n",
    "**Задание**: обучите три классификатора: \n",
    "\n",
    "1) на токенах с высокой частотой \n",
    "\n",
    "2) на токенах со средней частотой \n",
    "\n",
    "3) на токенах с низкой частотой\n",
    "\n",
    "Сравните полученные результаты, оцените какие токены наиболее важные для классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('не', 69472),\n",
       " ('и', 55166),\n",
       " ('в', 52902),\n",
       " ('я', 52818),\n",
       " ('RT', 38070),\n",
       " ('на', 35759),\n",
       " ('http', 32998),\n",
       " ('что', 31541),\n",
       " ('с', 27217),\n",
       " ('а', 26860)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dict_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "high = set([i[0].lower() for i in freq_dict_sorted if i[1] > 5000 and i[0].lower() not in noise])\n",
    "medium = set([i[0].lower() for i in freq_dict_sorted if 300 <= i[1] < 5000 and i[0].lower() not in noise])\n",
    "low = set([i[0].lower() for i in freq_dict_sorted if i[1] < 300 and i[0].lower() not in noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 600, 315191)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(high), len(medium), len(low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_preproc_custom(text, frequency):\n",
    "    text = re.sub('[{}]'.format(punctuation), '', text)\n",
    "    text = mystem_analyzer.lemmatize(text)\n",
    "    return [word for word in text if word in frequency]\n",
    "\n",
    "# def my_preproc_medium(text):\n",
    "#     text = re.sub('[{}]'.format(punctuation), '', text)\n",
    "#     text = mystem_analyzer.lemmatize(text)\n",
    "#     return [word for word in text if word in high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=lambda x: my_preproc_custom(x, high))\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.53      0.64     41938\n",
      "    positive       0.32      0.62      0.42     14771\n",
      "\n",
      "    accuracy                           0.55     56709\n",
      "   macro avg       0.56      0.57      0.53     56709\n",
      "weighted avg       0.67      0.55      0.58     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.47      0.65      0.55     20256\n",
      "    positive       0.75      0.59      0.66     36453\n",
      "\n",
      "    accuracy                           0.61     56709\n",
      "   macro avg       0.61      0.62      0.60     56709\n",
      "weighted avg       0.65      0.61      0.62     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=lambda x: my_preproc_custom(x, medium))\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.72      0.74     29278\n",
      "    positive       0.72      0.75      0.73     27431\n",
      "\n",
      "    accuracy                           0.74     56709\n",
      "   macro avg       0.74      0.74      0.74     56709\n",
      "weighted avg       0.74      0.74      0.74     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=lambda x: my_preproc_custom(x, low))\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Логично, что для обобщающей способности классификатора наиболее важны редкие токены"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV3fmzp-LVSU"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## О важности эксплоративного анализа\n",
    "\n",
    "Но иногда пунктуация бывает и не шумом -- главное отталкиваться от задачи. Что будет если вообще не убирать пунктуацию?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "qjkMxK9VLVSV",
    "outputId": "dfea56d5-4d92-4862-9788-29c8c8db29ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00     27876\n",
      "    positive       1.00      1.00      1.00     28833\n",
      "\n",
      "    accuracy                           1.00     56709\n",
      "   macro avg       1.00      1.00      1.00     56709\n",
      "weighted avg       1.00      1.00      1.00     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize)\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2fRbUAvLVSX"
   },
   "source": [
    "Шок! Стоило оставить пунктуацию -- и все метрики равны 1. Как это получилось? Среди неё были очень значимые токены (как вы думаете, какие?). Найдите фичи с самыми большими коэффициэнтами:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2.\n",
    "\n",
    "найти фичи с наибольшей значимостью, и вывести их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(')', 58.28809624959113),\n",
       " ('d', 26.901393960038757),\n",
       " ('dd', 10.622200359635846),\n",
       " ('^_^', 9.176231553227357),\n",
       " ('ddd', 8.134235722014749),\n",
       " ('-d', 7.273158195647029),\n",
       " ('*', 7.238638002729514),\n",
       " (':', 5.976265660415659),\n",
       " ('dddd', 4.802664458674663),\n",
       " ('ddddd', 2.9528262983503946),\n",
       " ('dddddd', 1.819570767682661),\n",
       " ('=^_^=', 1.7299306783359976),\n",
       " ('люблю', 1.6699543603888973),\n",
       " ('спасибо', 1.602575559114406),\n",
       " ('х', 1.5205892917957158),\n",
       " ('%', 1.3675840094458107),\n",
       " ('рождения', 1.2564531990855863),\n",
       " ('ахахах', 1.2177909883208682),\n",
       " ('ахах', 1.1850680145402248),\n",
       " ('dddddddd', 1.1774892492611042)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(i, j) for i, j in zip(vec.get_feature_names(), clf.coef_[0])], key=lambda x: -x[1])[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vtAyItvLVSb"
   },
   "source": [
    "Посмотрим, как один из супер-значительных токенов справится с классификацией безо всякого машинного обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "uqH07o-7LVSc",
    "outputId": "fad0a24a-98ee-4f84-8782-495548eb0fb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92     32756\n",
      "    positive       0.83      1.00      0.91     23953\n",
      "\n",
      "    accuracy                           0.91     56709\n",
      "   macro avg       0.92      0.93      0.91     56709\n",
      "weighted avg       0.93      0.91      0.92     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cool_token = ')'\n",
    "pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5THCOjMLVSg"
   },
   "source": [
    "## Символьные n-граммы\n",
    "\n",
    "Теперь в качестве фичей используем, например, униграммы символов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "AIUwDOabLVSh",
    "outputId": "54f129b1-994f-448e-e861-1912b4a21cdc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      1.00      0.99     27830\n",
      "    positive       1.00      0.99      1.00     28879\n",
      "\n",
      "    accuracy                           0.99     56709\n",
      "   macro avg       0.99      1.00      0.99     56709\n",
      "weighted avg       0.99      0.99      0.99     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(analyzer='char', ngram_range=(1, 1))\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_E0uPpgLVSj"
   },
   "source": [
    "В общем-то, теперь уже понятно, почему на этих данных здесь 1. Так или инчае, на символах классифицировать тоже можно: для некторых задач (например, для определения языка) фичи-символьные n-граммы решительно рулят.\n",
    "\n",
    "Ещё одна замечательная особенность фичей-символов: токенизация и лемматизация не нужна, можно использовать такой подход для языков, у которых нет готвых анализаторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.\n",
    "\n",
    "1) сравнить count/tf-idf/hashing векторайзеры/полносвязанную сетку (построить classification_report)\n",
    "\n",
    "2) подобрать оптимальный размер для hashing векторайзера \n",
    "\n",
    "3) убедиться что для сетки нет переобучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=noise)\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.76      0.78     29662\n",
      "    positive       0.75      0.80      0.78     27047\n",
      "\n",
      "    accuracy                           0.78     56709\n",
      "   macro avg       0.78      0.78      0.78     56709\n",
      "weighted avg       0.78      0.78      0.78     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77     28081\n",
      "    positive       0.78      0.78      0.78     28628\n",
      "\n",
      "    accuracy                           0.78     56709\n",
      "   macro avg       0.78      0.78      0.78     56709\n",
      "weighted avg       0.78      0.78      0.78     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=noise)\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.65      0.67     29665\n",
      "    positive       0.64      0.68      0.66     27044\n",
      "\n",
      "    accuracy                           0.67     56709\n",
      "   macro avg       0.67      0.67      0.67     56709\n",
      "weighted avg       0.67      0.67      0.67     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vec = HashingVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=noise, n_features=2**10)\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.76      0.76     28244\n",
      "    positive       0.76      0.78      0.77     28465\n",
      "\n",
      "    accuracy                           0.77     56709\n",
      "   macro avg       0.77      0.77      0.77     56709\n",
      "weighted avg       0.77      0.77      0.77     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vec = HashingVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=noise, n_features=2**20)\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.76      0.77     28466\n",
      "    positive       0.76      0.78      0.77     28243\n",
      "\n",
      "    accuracy                           0.77     56709\n",
      "   macro avg       0.77      0.77      0.77     56709\n",
      "weighted avg       0.77      0.77      0.77     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vec = HashingVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=noise, n_features=2**25)\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимальный размер векторайзера = 2**20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<170125x1048576 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1393315 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170125, 1048576)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hashing_dataset(Dataset):\n",
    "    def __init__(self, df, target):\n",
    "        self.df = df\n",
    "        self.target = target.replace({'positive': 1, 'negative': 0})\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.df.getrow(idx).toarray()[0], dtype=torch.float32), torch.tensor(self.target.iloc[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_, y_test_ = y_train.copy(), y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<170125x1048576 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1394046 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = hashing_dataset(bow, y_train_)\n",
    "test_df = hashing_dataset(vec.transform(x_test), y_test_)\n",
    "\n",
    "train_loader = DataLoader(train_df, batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(test_df, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_dim = 1048576\n",
    "        self.fc1 = nn.Linear(self.input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.l_relu = nn.LeakyReLU()\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.l_relu(x)\n",
    "        x = self.dp(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.l_relu(x)\n",
    "        x = self.dp(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "loss_func = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm_notebook(range(5)):\n",
    "    for x_train, y_train in tqdm_notebook(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x_train)\n",
    "        loss = loss_func(pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Этот код выполнил в google.colab, загружаю параметры сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoint.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array([])\n",
    "y = np.array([])\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test in test_loader:\n",
    "        pred = np.append(pred, model(x_test))\n",
    "        y = np.append(y, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.69      0.81     39969\n",
      "           1       0.57      0.99      0.72     16740\n",
      "\n",
      "    accuracy                           0.78     56709\n",
      "   macro avg       0.78      0.84      0.77     56709\n",
      "weighted avg       0.87      0.78      0.79     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pred.astype(int).tolist(), y.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кажется, что получилось чуть лучше, чем логрег"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gJABxhalLVQu",
    "IaQMCGHFLVQ6",
    "5AJk1B39LVRP",
    "RJlvqWuALVRs",
    "rck5OVqhLVSA",
    "mV3fmzp-LVSU",
    "H5THCOjMLVSg",
    "02s2Vh7MLVSj",
    "b1khxRFDLVSm",
    "sfUmWcAQLVSt",
    "BxvtN-3zLVS5",
    "gyrHhYkgLVTB"
   ],
   "name": "sem1_intro_common.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
