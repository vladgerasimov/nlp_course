{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUotKHRULVPD"
   },
   "source": [
    "# Инструменты для работы с языком "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ba5Z02VLVPK"
   },
   "source": [
    "## Задача: классификация твитов по тональности\n",
    "\n",
    "У нас есть датасет из твитов, про каждый указано, как он эмоционально окрашен: положительно или отрицательно. Задача: предсказывать эмоциональную окраску.\n",
    "\n",
    "Скачиваем куски датасета ([источник](http://study.mokoron.com/)): [положительные](https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0), [отрицательные](https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zHB70n5LVPN",
    "outputId": "76e9cc41-c912-464c-a06e-e02879e69c08",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# если у вас линукс / мак / collab или ещё какая-то среда, в которой работает wget, можно так:\n",
    "# !wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
    "# !wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "J5YiZNCPLVPe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DFLtXAZ-LVPq"
   },
   "outputs": [],
   "source": [
    "# считываем данные и заполняем общий датасет\n",
    "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
    "positive['label'] = ['positive'] * len(positive)\n",
    "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
    "negative['label'] = ['negative'] * len(negative)\n",
    "df = positive.append(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "j1AEISlBLVP0",
    "outputId": "443eadf2-9df4-4507-f2a5-a64f7968182f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111918</th>\n",
       "      <td>Но не каждый хочет что то исправлять:( http://...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111919</th>\n",
       "      <td>скучаю так :-( только @taaannyaaa вправляет мо...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111920</th>\n",
       "      <td>Вот и в школу, в говно это идти уже надо(</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111921</th>\n",
       "      <td>RT @_Them__: @LisaBeroud Тауриэль, не грусти :...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111922</th>\n",
       "      <td>Такси везет меня на работу. Раздумываю приплат...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     label\n",
       "111918  Но не каждый хочет что то исправлять:( http://...  negative\n",
       "111919  скучаю так :-( только @taaannyaaa вправляет мо...  negative\n",
       "111920          Вот и в школу, в говно это идти уже надо(  negative\n",
       "111921  RT @_Them__: @LisaBeroud Тауриэль, не грусти :...  negative\n",
       "111922  Такси везет меня на работу. Раздумываю приплат...  negative"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZWta7oDgLVP8"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAapBC7VLVQC"
   },
   "source": [
    "## Baseline: классификация необработанных n-грамм\n",
    "\n",
    "### Векторизаторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "M-AvVt8XLVQD"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # можно заменить на любимый классификатор\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSuoVoxcLVQI"
   },
   "source": [
    "Что такое n-граммы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zeNA7732LVQJ"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NeApDOmrLVQN",
    "outputId": "d8c763fe-2658-47ac-fcee-5b7bbe49f0d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если',), ('б',), ('мне',), ('платили',), ('каждый',), ('раз',)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Если б мне платили каждый раз'.split()\n",
    "list(ngrams(sent, 1)) # униграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OPAS0fS-LVQQ",
    "outputId": "aa3ae031-c661-4639-b7ab-f93ba1b6cee5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б'),\n",
       " ('б', 'мне'),\n",
       " ('мне', 'платили'),\n",
       " ('платили', 'каждый'),\n",
       " ('каждый', 'раз')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sent, 2)) # биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d77jmVPhLVQU",
    "outputId": "c8de801b-8efc-437e-8673-4e9e31665ea5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б', 'мне'),\n",
       " ('б', 'мне', 'платили'),\n",
       " ('мне', 'платили', 'каждый'),\n",
       " ('платили', 'каждый', 'раз')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sent, 3)) # триграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5xXTBrGELVQX",
    "outputId": "f5b423d9-db6b-4efa-8bfa-a446a55ee018"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б', 'мне', 'платили', 'каждый'),\n",
       " ('б', 'мне', 'платили', 'каждый', 'раз')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sent, 5)) # ... пентаграммы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHGJBEm-LVQb"
   },
   "source": [
    "Самый простой способ извлечь фичи из текстовых данных -- векторизаторы: `CountVectorizer` и `TfidfVectorizer`\n",
    "\n",
    "Объект `CountVectorizer` делает простую вещь:\n",
    "* строит для каждого документа (каждой пришедшей ему строки) вектор размерности `n`, где `n` -- количество слов или n-грам во всём корпусе\n",
    "* заполняет каждый i-тый элемент количеством вхождений слова в данный документ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eMqZFBTgLVQb"
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1))\n",
    "bow = vec.fit_transform(x_train) # bow -- bag of words (мешок слов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZkpqVtILVQe"
   },
   "source": [
    "ngram_range отвечает за то, какие n-граммы мы используем в качестве фичей:<br/>\n",
    "ngram_range=(1, 1) -- униграммы<br/>\n",
    "ngram_range=(3, 3) -- триграммы<br/>\n",
    "ngram_range=(1, 3) -- униграммы, биграммы и триграммы.\n",
    "\n",
    "В vec.vocabulary_ лежит словарь: мэппинг слов к их индексам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWRtOSzKLVQf",
    "outputId": "668857b0-def2-4547-8fd5-014fe3858bec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('только', 222370),\n",
       " ('симсими', 209371),\n",
       " ('может', 159561),\n",
       " ('сделать', 207793),\n",
       " ('мне', 159206),\n",
       " ('кучу', 151047),\n",
       " ('комплиментов', 147045),\n",
       " ('поднять', 185335),\n",
       " ('настроение', 164647),\n",
       " ('onlyyyhardcore', 66620)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vec.vocabulary_.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hkmX3iBbLVQi",
    "outputId": "5bbb432e-1c45-422a-edc2-60841ef0ee33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.76      0.77     28259\n",
      "    positive       0.77      0.78      0.77     28450\n",
      "\n",
      "    accuracy                           0.77     56709\n",
      "   macro avg       0.77      0.77      0.77     56709\n",
      "weighted avg       0.77      0.77      0.77     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAhgaYgqLVQp"
   },
   "source": [
    "Попробуем сделать то же самое для триграмм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPWXlh6ALVQq",
    "outputId": "094a7189-bc36-42df-81ee-f599a206ca0c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.47      0.72      0.57     18250\n",
      "    positive       0.82      0.61      0.70     38459\n",
      "\n",
      "    accuracy                           0.65     56709\n",
      "   macro avg       0.64      0.66      0.63     56709\n",
      "weighted avg       0.71      0.65      0.66     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(3, 3))\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnfyJkzTLVQu"
   },
   "source": [
    "(как вы думаете, почему в результатах теперь такой разброс по сравнению с униграммами?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJABxhalLVQu"
   },
   "source": [
    "## TF-IDF векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LJES2s-LVQv"
   },
   "source": [
    "`TfidfVectorizer` делает то же, что и `CountVectorizer`, но в качестве значений – tf-idf каждого слова.\n",
    "\n",
    "Как считается tf-idf:\n",
    "\n",
    "TF (term frequency) – относительная частотность слова в документе:\n",
    "$$ TF(t,d) = \\frac{n_t}{\\sum_k n_k} $$\n",
    "\n",
    "`t` -- слово (term), `d` -- документ, $n_t$ -- количество вхождений слова, $n_k$ -- количество вхождений остальных слов\n",
    "\n",
    "IDF (inverse document frequency) – обратная частота документов, в которых есть это слово:\n",
    "$$ IDF(t, D) = \\mbox{log} \\frac{|D|}{|{d : t \\in d}|} $$\n",
    "\n",
    "`t` -- слово (term), `D` -- коллекция документов\n",
    "\n",
    "Перемножаем их:\n",
    "$$TFIDF_(t,d,D) = TF(t,d) \\times IDF(i, D)$$\n",
    "\n",
    "Сакральный смысл – если слово часто встречается в одном документе, но в целом по корпусу встречается в небольшом \n",
    "количестве документов, у него высокий TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "FmEcRD28LVQ0"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWLhMl9xLVQ3",
    "outputId": "054e5662-1c41-42f6-92e4-ce0194317ce8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.77      0.75     26747\n",
      "    positive       0.78      0.75      0.77     29962\n",
      "\n",
      "    accuracy                           0.76     56709\n",
      "   macro avg       0.76      0.76      0.76     56709\n",
      "weighted avg       0.76      0.76      0.76     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1, 1))\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTODTRnKLVQ6"
   },
   "source": [
    "В этот раз получилось хуже :( Вернёмся к `CountVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8v9Scpn9Y0M"
   },
   "source": [
    "## PMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVRqLcSY0etj"
   },
   "source": [
    "Можно оценить взаимосвязь слов в корпусе и понять, какие биграммы наиболее часто встречаются в тексте. Для этого можно использовать метрику PMI (Pointwise Mutual Information) - поточечная взаимная информация. Метрика PMI для двух слов вычисляется по формуле:\n",
    "\n",
    "$$pmi(x; y) = log \\frac{p(x,y)}{p(x)p(y)} $$\n",
    "\n",
    "Здесь p(y|x) - вероятность встретить слово $y$ после $x$, $p(y)$ - вероятность встретить слово $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXgDwf6W6Kk5"
   },
   "source": [
    "Оценим важность биграмм в нашем обучающем корпусе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKmiOEaW53F9",
    "outputId": "96a85968-e0cb-4b17-cd62-bc3de5e3e9b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to /Users/vlad/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n"
     ]
    }
   ],
   "source": [
    "from nltk import collocations \n",
    "import nltk\n",
    "nltk.download('genesis')\n",
    "\n",
    "print(type(nltk.corpus.genesis.words('english-web.txt')))\n",
    "bigram_measures = collocations.BigramAssocMeasures()\n",
    "# bigram_finder.apply_freq_filter(5)\n",
    "bigram_finder = collocations.BigramCollocationFinder.from_documents([nltk.word_tokenize(x) for x in x_train])\n",
    "bigrams = bigram_finder.nbest(bigram_measures.pmi, 100)\n",
    "# print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_h4Cq1PUTTc-"
   },
   "source": [
    "Можно рассмотреть другие метрики оценки важности биграмм, например, метрику правдоподобия (подробнее про вычисление метрики можно посмотреть [здесь (пункт 5.3.4)](http://www.corpus.unam.mx/cursoenah/ManningSchutze_1999_FoundationsofStatisticalNaturalLanguageProcessing.pdf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lTOJg4KoOo84",
    "outputId": "45d38953-84e0-4a65-fccd-1e96fd83c3f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('(', '('), ('RT', '@'), (')', ')'), ('http', ':'), ('!', '!'), (':', 'D'), ('у', 'меня'), (':', '('), (',', 'а'), (',', 'что'), (',', 'но'), (')', 'http'), ('*', '*'), ('у', 'нас'), (':', ')'), ('(', ','), (':', '-'), ('не', 'могу'), (',', '('), (',', ')'), ('?', '?'), (')', ','), (',', ':'), ('@', '('), (',', ','), ('(', ':'), (':', ','), ('@', ')'), ('&', 'lt'), ('@', ','), ('со', 'мной'), ('@', ':'), (':', ':'), ('(', '@'), ('gt', ';'), ('новый', 'год'), (';', ')'), (')', ':'), (':', '*'), ('не', 'знаю'), (',', '@'), ('а', 'я'), ('@', '@'), ('сих', 'пор'), ('У', 'меня'), ('lt', ';'), ('потому', 'что'), (',', 'когда'), ('&', 'gt'), ('у', 'тебя'), (';', '('), ('все', 'равно'), (',', 'как'), ('с', 'тобой'), ('в', 'школу'), ('(', 'http'), ('&', 'amp'), (')', '@'), (',', 'я'), ('ничего', 'не'), ('Как', 'же'), ('Доброе', 'утро'), ('-', ')'), ('я', 'не'), (':', 'DD'), ('не', '('), ('самом', 'деле'), ('до', 'сих'), ('amp', ';'), ('не', ')'), ('--', '--'), ('как', 'же'), ('(', '!'), ('что', 'я'), (',', '!'), ('с', 'кем'), ('никто', 'не'), ('D', 'http'), ('.', 'А'), (',', 'чтобы'), ('не', ':'), ('об', 'этом'), ('!', ','), (':', '!'), ('=', ')'), ('а', 'потом'), (':', '|'), ('?', '—'), ('никогда', 'не'), ('и', ')'), ('Новый', 'Год'), (',', '.'), ('.', ','), ('в', 'этом'), ('#', 'євромайдан'), ('@', 'не'), ('“', '@'), ('не', '@'), (':', '.'), ('=', '(')]\n"
     ]
    }
   ],
   "source": [
    "bigrams = bigram_finder.nbest(bigram_measures.likelihood_ratio, 100)\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfjCYZa8TeX_"
   },
   "source": [
    "Как можно заметить, немаловажную роль в текстах занимает пунктуация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AJk1B39LVRP"
   },
   "source": [
    "## Стоп-слова и пунктуация\n",
    "\n",
    "*Стоп-слова* -- это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конретном документе, то есть играют роль шума. Поэтому их принято убирать. По той же причине убирают и пунктуацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpWhsTuRLVRP",
    "outputId": "1cd18efe-a3cd-4f56-ec4c-bec8b6ee442e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/vlad/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# у вас здесь, вероятно, выскочит ошибка и надо будет загрузить стоп слова (в тексте ошибки написано, как)\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "OdRF7rlyLVRS",
    "outputId": "dd4ce4f0-13d0-4b21-a3a1-b9ecb281894f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "OfXiH98XLVRV"
   },
   "outputs": [],
   "source": [
    "noise = stopwords.words('russian') + list(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtiIhHDMLVRY"
   },
   "source": [
    "В векторизаторах за стоп-слова, логичным образом, отвечает аргумент `stop_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZnbarm_LVRY",
    "outputId": "88f40278-165a-46ad-a75e-2e5d8e7e3a4d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.77      0.78     29125\n",
      "    positive       0.76      0.80      0.78     27584\n",
      "\n",
      "    accuracy                           0.78     56709\n",
      "   macro avg       0.78      0.78      0.78     56709\n",
      "weighted avg       0.78      0.78      0.78     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=noise)\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wr934O7yLVRb"
   },
   "source": [
    "Получилось чууть лучше. Что ещё можно сделать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7O_oD1fLVRc"
   },
   "source": [
    "## Лемматизация\n",
    "\n",
    "Лемматизация – это сведение разных форм одного слова к начальной форме – *лемме*. Почему это хорошо?\n",
    "* Во-первых, мы хотим рассматривать как отдельную фичу каждое *слово*, а не каждую его отдельную форму.\n",
    "* Во-вторых, некоторые стоп-слова стоят только в начальной форме, и без лематизации выкидываем мы только её.\n",
    "\n",
    "Для русского есть два хороших лемматизатора: mystem и pymorphy:\n",
    "\n",
    "### [Mystem](https://tech.yandex.ru/mystem/)\n",
    "Как с ним работать:\n",
    "* можно скачать mystem и запускать [из терминала с разными параметрами](https://tech.yandex.ru/mystem/doc/)\n",
    "* [pymystem3](https://pythonhosted.org/pymystem3/pymystem3.html) - обертка для питона, работает медленнее, но это удобно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96HdoB7zLVRc",
    "outputId": "987397bc-55cb-4830-c361-5568f1f2e015",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
    "# !tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
    "# !cp mystem /bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "kzQwGwAaZWV5"
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "mystem_analyzer = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_w-_fkNtLVRf"
   },
   "source": [
    "Мы инициализировали Mystem c дефолтными параметрами. А вообще параметры есть такие:\n",
    "* mystem_bin - путь к `mystem`, если их несколько\n",
    "* grammar_info - нужна ли грамматическая информация или только леммы (по дефолту нужна)\n",
    "* disambiguation - нужно ли снятие омонимии - дизамбигуация (по дефолту нужна)\n",
    "* entire_input - нужно ли сохранять в выводе все (пробелы всякие, например), или можно выкинуть (по дефолту оставляется все)\n",
    "\n",
    "Методы Mystem принимают строку, токенизатор вшит внутри. Можно, конечно, и пословно анализировать, но тогда он не сможет учитывать контекст.\n",
    "\n",
    "Можно просто лемматизировать текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 'Если б мне платили каждый раз'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fjHHLQv9txDq",
    "outputId": "35bcb3cc-7853-48bf-d38d-04157f45221d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['если', ' ', 'б', ' ', 'я', ' ', 'платить', ' ', 'каждый', ' ', 'раз', '\\n']\n"
     ]
    }
   ],
   "source": [
    "print(mystem_analyzer.lemmatize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RI1eftjkLVRi"
   },
   "source": [
    "А можно получить грамматическую информацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j4MLqlZnxNEj",
    "outputId": "ffb676bb-932e-4579-817a-c15616431521"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'если', 'wt': 1, 'gr': 'CONJ='}], 'text': 'Если'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'б', 'wt': 0.9980299343, 'gr': 'PART='}], 'text': 'б'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'я', 'wt': 1, 'gr': 'SPRO,ед,1-л=(пр|дат)'}],\n",
       "  'text': 'мне'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'платить', 'wt': 1, 'gr': 'V,несов,пе=прош,мн,изъяв'}],\n",
       "  'text': 'платили'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'каждый',\n",
       "    'wt': 0.9985975799,\n",
       "    'gr': 'APRO=(вин,ед,муж,неод|им,ед,муж)'}],\n",
       "  'text': 'каждый'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'раз',\n",
       "    'wt': 0.9278416023,\n",
       "    'gr': 'S,муж,неод=(вин,ед|род,мн|им,ед)'}],\n",
       "  'text': 'раз'},\n",
       " {'text': '\\n'}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem_analyzer.analyze(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADcGtz4JLVRl"
   },
   "source": [
    "Давайте терепь используем лемматизатор майстема в качестве токенизатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "x48Q56tiLVRn"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def my_preproc(text):\n",
    "    text = re.sub('[{}]'.format(punctuation), '', text)\n",
    "    text = mystem_analyzer.lemmatize(text)\n",
    "    return [word for word in text if word not in stopwords.words('russian') + [' ', '\\n']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEwOQTJPLVRq",
    "outputId": "b1047a05-9fa7-4994-c947-578cfc4d9825"
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=my_preproc)\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.74      0.76     29051\n",
      "    positive       0.74      0.77      0.75     27658\n",
      "\n",
      "    accuracy                           0.76     56709\n",
      "   macro avg       0.76      0.76      0.76     56709\n",
      "weighted avg       0.76      0.76      0.76     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJlvqWuALVRs"
   },
   "source": [
    "### [Pymorphy](http://pymorphy2.readthedocs.io/en/latest/)\n",
    "Это модуль на питоне, довольно быстрый и с кучей функций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tHDkurN1zf7g",
    "outputId": "c9934446-bf72-4603-8a51-9f6ebf406e44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy2 in /opt/anaconda3/lib/python3.8/site-packages (0.9.1)\r\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /opt/anaconda3/lib/python3.8/site-packages (from pymorphy2) (2.4.417127.4579844)\r\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /opt/anaconda3/lib/python3.8/site-packages (from pymorphy2) (0.7.2)\r\n",
      "Requirement already satisfied: docopt>=0.6 in /opt/anaconda3/lib/python3.8/site-packages (from pymorphy2) (0.6.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "7SlwsLU7LVRt"
   },
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "pymorphy2_analyzer = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qaz0x7frLVRw"
   },
   "source": [
    "pymorphy2 работает с отдельными словами. Если дать ему на вход предложение - он его просто не лемматизирует, т.к. не понимает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdf6XoEbLVRw",
    "outputId": "a1984e06-dbeb-4377-896d-b7014b6b84c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='платили', tag=OpencorporaTag('VERB,impf,tran plur,past,indc'), normal_form='платить', score=1.0, methods_stack=((DictionaryAnalyzer(), 'платили', 2472, 10),))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana = pymorphy2_analyzer.parse(sent[3])\n",
    "ana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "0KuHQGPgLVRz",
    "outputId": "3cdbe79d-ec3f-4a07-ff3a-52e8810face1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'платить'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gg0EASPcLVR8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFTkF8xUARlS"
   },
   "source": [
    "### [Natasha](https://github.com/natasha/)\n",
    "\n",
    "В библиотеке natasha реализовано множество полезных библиотек для русского языка: разбиение на токены и предложения, русскоязычные word embeddings, морфологический, синтаксический анализ, лемматизация, извлечение именованных сущностей и т.д. Модуль библиотеки Razdel, основанный на правилах, предназначен для разбиения текста на токены и предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CVeDxeIA6rg",
    "outputId": "ff583009-ae7a-4b68-b6a7-ca1ba48c0732"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: razdel in /opt/anaconda3/lib/python3.8/site-packages (0.5.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MOTkw9MpAnNN",
    "outputId": "51fc6e54-3b82-4c1c-91c2-5b6a60f6304f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0, 13, 'Кружка-термос'),\n",
       " Substring(14, 16, 'на'),\n",
       " Substring(17, 20, '0.5'),\n",
       " Substring(20, 21, 'л'),\n",
       " Substring(22, 23, '('),\n",
       " Substring(23, 28, '50/64'),\n",
       " Substring(29, 32, 'см³'),\n",
       " Substring(32, 33, ','),\n",
       " Substring(34, 37, '516'),\n",
       " Substring(37, 38, ';'),\n",
       " Substring(38, 41, '...'),\n",
       " Substring(41, 42, ')')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from razdel import tokenize\n",
    "\n",
    "tokens = list(tokenize('Кружка-термос на 0.5л (50/64 см³, 516;...)'))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ftx-WzUbBCpO",
    "outputId": "40c67fa4-fab6-4c1b-f651-06951a38798e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Кружка-термос',\n",
       " 'на',\n",
       " '0.5',\n",
       " 'л',\n",
       " '(',\n",
       " '50/64',\n",
       " 'см³',\n",
       " ',',\n",
       " '516',\n",
       " ';',\n",
       " '...',\n",
       " ')']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_.text for _ in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyhsQp4MGbW8",
    "outputId": "55f84576-aaad-4a68-c5d9-38dc0ea2f721"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting natasha\n",
      "  Downloading natasha-1.4.0-py3-none-any.whl (34.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 34.4 MB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting slovnet>=0.3.0\n",
      "  Downloading slovnet-0.5.0-py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 3.2 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: razdel>=0.5.0 in /opt/anaconda3/lib/python3.8/site-packages (from natasha) (0.5.0)\n",
      "Collecting ipymarkup>=0.8.0\n",
      "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
      "Collecting navec>=0.9.0\n",
      "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pymorphy2 in /opt/anaconda3/lib/python3.8/site-packages (from natasha) (0.9.1)\n",
      "Collecting yargy>=0.14.0\n",
      "  Downloading yargy-0.15.0-py3-none-any.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 238 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/anaconda3/lib/python3.8/site-packages (from slovnet>=0.3.0->natasha) (1.18.5)\n",
      "Requirement already satisfied: intervaltree>=3 in /opt/anaconda3/lib/python3.8/site-packages (from ipymarkup>=0.8.0->natasha) (3.1.0)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /opt/anaconda3/lib/python3.8/site-packages (from pymorphy2->natasha) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /opt/anaconda3/lib/python3.8/site-packages (from pymorphy2->natasha) (2.4.417127.4579844)\n",
      "Requirement already satisfied: docopt>=0.6 in /opt/anaconda3/lib/python3.8/site-packages (from pymorphy2->natasha) (0.6.2)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /opt/anaconda3/lib/python3.8/site-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.2.2)\n",
      "Installing collected packages: navec, slovnet, ipymarkup, yargy, natasha\n",
      "Successfully installed ipymarkup-0.9.0 natasha-1.4.0 navec-0.10.0 slovnet-0.5.0 yargy-0.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install natasha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMO3jsqLKSIV"
   },
   "source": [
    "С помощью библиотеки natasha можно также лемматизировать тексты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "vJZgfRnvIS2q"
   },
   "outputs": [],
   "source": [
    "from natasha import Doc, MorphVocab, Segmenter, NewsEmbedding, NewsMorphTagger\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "\n",
    "def natasha_lemmatize(text):\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "    return {_.text: _.lemma for _ in doc.tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBtlnYlFBOKv",
    "outputId": "47a9e7d6-7f03-4e93-e57c-84dce5657d97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Посол': 'посол',\n",
       " 'Израиля': 'израиль',\n",
       " 'на': 'на',\n",
       " 'Украине': 'украина',\n",
       " 'Йоэль': 'йоэль',\n",
       " 'Лион': 'лион',\n",
       " 'признался': 'признаться',\n",
       " ',': ',',\n",
       " 'что': 'что',\n",
       " 'пришел': 'прийти',\n",
       " 'в': 'в',\n",
       " 'шок': 'шок',\n",
       " 'узнав': 'узнать',\n",
       " 'о': 'о',\n",
       " 'решении': 'решение',\n",
       " 'властей': 'власть',\n",
       " 'Львовской': 'львовский',\n",
       " 'области': 'область',\n",
       " 'объявить': 'объявить',\n",
       " '2019': '2019',\n",
       " 'год': 'год',\n",
       " 'годом': 'год',\n",
       " 'лидера': 'лидер',\n",
       " 'запрещенной': 'запретить',\n",
       " 'России': 'россия',\n",
       " 'Организации': 'организация',\n",
       " 'украинских': 'украинский',\n",
       " 'националистов': 'националист',\n",
       " '(': '(',\n",
       " 'ОУН': 'оун',\n",
       " ')': ')',\n",
       " 'Степана': 'степан',\n",
       " 'Бандеры': 'бандера',\n",
       " '.': '.',\n",
       " 'Свое': 'свой',\n",
       " 'заявление': 'заявление',\n",
       " 'он': 'он',\n",
       " 'разместил': 'разместить',\n",
       " 'Twitter': 'twitter',\n",
       " '«': '«',\n",
       " 'Я': 'я',\n",
       " 'не': 'не',\n",
       " 'могу': 'мочь',\n",
       " 'понять': 'понять',\n",
       " 'как': 'как',\n",
       " 'прославление': 'прославление',\n",
       " 'тех': 'тот',\n",
       " 'кто': 'кто',\n",
       " 'непосредственно': 'непосредственно',\n",
       " 'принимал': 'принимать',\n",
       " 'участие': 'участие',\n",
       " 'ужасных': 'ужасный',\n",
       " 'антисемитских': 'антисемитский',\n",
       " 'преступлениях': 'преступление',\n",
       " 'помогает': 'помогать',\n",
       " 'бороться': 'бороться',\n",
       " 'с': 'с',\n",
       " 'антисемитизмом': 'антисемитизм',\n",
       " 'и': 'и',\n",
       " 'ксенофобией': 'ксенофобия',\n",
       " 'Украина': 'украина',\n",
       " 'должна': 'должный',\n",
       " 'забывать': 'забывать',\n",
       " 'совершенных': 'совершить',\n",
       " 'против': 'против',\n",
       " 'евреев': 'еврей',\n",
       " 'никоим': 'никой',\n",
       " 'образом': 'образ',\n",
       " 'отмечать': 'отмечать',\n",
       " 'их': 'они',\n",
       " 'через': 'через',\n",
       " 'почитание': 'почитание',\n",
       " 'исполнителей': 'исполнитель',\n",
       " '»': '»',\n",
       " '—': '—',\n",
       " 'написал': 'написать',\n",
       " 'дипломат': 'дипломат',\n",
       " '11': '11',\n",
       " 'декабря': 'декабрь',\n",
       " 'Львовский': 'львовский',\n",
       " 'областной': 'областной',\n",
       " 'совет': 'совет',\n",
       " 'принял': 'принять',\n",
       " 'решение': 'решение',\n",
       " 'провозгласить': 'провозгласить',\n",
       " 'регионе': 'регион',\n",
       " 'связи': 'связь',\n",
       " 'празднованием': 'празднование',\n",
       " '110-летия': '110-летие',\n",
       " 'со': 'с',\n",
       " 'дня': 'день',\n",
       " 'рождения': 'рождение',\n",
       " 'Бандера': 'бандера',\n",
       " 'родился': 'родиться',\n",
       " '1': '1',\n",
       " 'января': 'январь',\n",
       " '1909': '1909',\n",
       " 'года': 'год',\n",
       " 'В': 'в',\n",
       " 'июле': 'июль',\n",
       " 'аналогичное': 'аналогичный',\n",
       " 'Житомирский': 'житомирский',\n",
       " 'начале': 'начало',\n",
       " 'месяца': 'месяц',\n",
       " 'предложением': 'предложение',\n",
       " 'к': 'к',\n",
       " 'президенту': 'президент',\n",
       " 'страны': 'страна',\n",
       " 'Петру': 'петр',\n",
       " 'Порошенко': 'порошенко',\n",
       " 'вернуть': 'вернуть',\n",
       " 'Бандере': 'бандера',\n",
       " 'звание': 'звание',\n",
       " 'Героя': 'герой',\n",
       " 'Украины': 'украина',\n",
       " 'обратились': 'обратиться',\n",
       " 'депутаты': 'депутат',\n",
       " 'Верховной': 'верховный',\n",
       " 'Рады': 'рада',\n",
       " 'Парламентарии': 'парламентарий',\n",
       " 'уверены': 'уверить',\n",
       " 'признание': 'признание',\n",
       " 'национальным': 'национальный',\n",
       " 'героем': 'герой',\n",
       " 'поможет': 'помочь',\n",
       " 'борьбе': 'борьба',\n",
       " 'подрывной': 'подрывной',\n",
       " 'деятельностью': 'деятельность',\n",
       " 'информационном': 'информационный',\n",
       " 'поле': 'поле',\n",
       " 'а': 'а',\n",
       " 'также': 'также',\n",
       " 'остановит': 'остановить',\n",
       " 'распространение': 'распространение',\n",
       " 'мифов': 'миф',\n",
       " 'созданных': 'создать',\n",
       " 'российской': 'российский',\n",
       " 'пропагандой': 'пропаганда',\n",
       " 'Степан': 'степан',\n",
       " '1909-1959': '1909-1959',\n",
       " 'был': 'быть',\n",
       " 'одним': 'один',\n",
       " 'из': 'из',\n",
       " 'лидеров': 'лидер',\n",
       " 'выступающей': 'выступать',\n",
       " 'за': 'за',\n",
       " 'создание': 'создание',\n",
       " 'независимого': 'независимый',\n",
       " 'государства': 'государство',\n",
       " 'территориях': 'территория',\n",
       " 'украиноязычным': 'украиноязычный',\n",
       " 'населением': 'население',\n",
       " '2010': '2010',\n",
       " 'году': 'год',\n",
       " 'период': 'период',\n",
       " 'президентства': 'президентство',\n",
       " 'Виктора': 'виктор',\n",
       " 'Ющенко': 'ющенко',\n",
       " 'посмертно': 'посмертно',\n",
       " 'признан': 'признать',\n",
       " 'Героем': 'герой',\n",
       " 'однако': 'однако',\n",
       " 'впоследствии': 'впоследствии',\n",
       " 'это': 'это',\n",
       " 'было': 'быть',\n",
       " 'отменено': 'отменить',\n",
       " 'судом': 'суд'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Посол Израиля на Украине Йоэль Лион признался, что пришел в шок, узнав о решении властей Львовской области объявить 2019 год годом лидера запрещенной в России Организации украинских националистов (ОУН) Степана Бандеры. Свое заявление он разместил в Twitter. «Я не могу понять, как прославление тех, кто непосредственно принимал участие в ужасных антисемитских преступлениях, помогает бороться с антисемитизмом и ксенофобией. Украина не должна забывать о преступлениях, совершенных против украинских евреев, и никоим образом не отмечать их через почитание их исполнителей», — написал дипломат. 11 декабря Львовский областной совет принял решение провозгласить 2019 год в регионе годом Степана Бандеры в связи с празднованием 110-летия со дня рождения лидера ОУН (Бандера родился 1 января 1909 года). В июле аналогичное решение принял Житомирский областной совет. В начале месяца с предложением к президенту страны Петру Порошенко вернуть Бандере звание Героя Украины обратились депутаты Верховной Рады. Парламентарии уверены, что признание Бандеры национальным героем поможет в борьбе с подрывной деятельностью против Украины в информационном поле, а также остановит «распространение мифов, созданных российской пропагандой». Степан Бандера (1909-1959) был одним из лидеров Организации украинских националистов, выступающей за создание независимого государства на территориях с украиноязычным населением. В 2010 году в период президентства Виктора Ющенко Бандера был посмертно признан Героем Украины, однако впоследствии это решение было отменено судом. '\n",
    "\n",
    "natasha_lemmatize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rck5OVqhLVSA"
   },
   "source": [
    "### mystem vs. pymorphy vs. natasha\n",
    "\n",
    "1) *Мы надеемся, что вы пользуетесь линуксом*, но mystem работает невероятно медленно под windows на больших текстах.\n",
    "\n",
    "2) *Снятие омонимии*. Mystem умеет снимать омонимию по контексту (хотя не всегда преуспевает), pymorphy2 берет на вход одно слово и соответственно вообще не умеет дизамбигуировать по контексту, natasha тоже с этим тоже не справляется успешно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "kH2GQ4ddLVSB"
   },
   "outputs": [],
   "source": [
    "homonym1 = 'За время обучения я прослушал больше сорока курсов.'\n",
    "homonym2 = 'Сорока своровала блестящее украшение со стола.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WwF-XsjeI3eX",
    "outputId": "34e9f131-3af5-4a87-9a8c-aa51d6e076ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analysis': [{'lex': 'сорок', 'wt': 0.8710292664, 'gr': 'NUM=(пр|дат|род|твор)'}], 'text': 'сорока'}\n",
      "{'analysis': [{'lex': 'сорока', 'wt': 0.1210970041, 'gr': 'S,жен,од=им,ед'}], 'text': 'Сорока'}\n"
     ]
    }
   ],
   "source": [
    "mystem_analyzer = Mystem() # инициализирую объект с дефолтными параметрами\n",
    "\n",
    "print(mystem_analyzer.analyze(homonym1)[-5])\n",
    "print(mystem_analyzer.analyze(homonym2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t9jezRVlFmDo",
    "outputId": "e2224fd9-09e3-428e-fa6a-1af94dcb2ca8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'За': 'за', 'время': 'время', 'обучения': 'обучение', 'я': 'я', 'прослушал': 'прослушать', 'больше': 'большой', 'сорока': 'сорок', 'курсов': 'курс', '.': '.'}\n"
     ]
    }
   ],
   "source": [
    "print(natasha_lemmatize(homonym1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SXjGBQPoI9gl",
    "outputId": "049fe8af-e0c5-499f-cc10-6d05b047a168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Сорока': 'сорок', 'своровала': 'своровать', 'блестящее': 'блестящий', 'украшение': 'украшение', 'со': 'с', 'стола': 'стол', '.': '.'}\n"
     ]
    }
   ],
   "source": [
    "print(natasha_lemmatize(homonym2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aP5qFnilLVSI"
   },
   "source": [
    "## Словарь, закон Ципфа и закон Хипса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1umtd3OLVSI"
   },
   "source": [
    "Закон Ципфа -- эмпирическая закономерность: если все слова корпуса текста упорядочить по убыванию частоты их использования, то частота n-го слова в таком списке окажется приблизительно обратно пропорциональной его порядковому номеру n. Иными словами, частотность слов убывает очень быстро."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "lY0cWJ7eLVSJ"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vIjqSVjpLVSL",
    "outputId": "a75ec748-ab21-4bd0-cd41-5a9fa8362b22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2870536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['first_timee', 'хоть', 'я', 'и', 'школота', 'но', 'поверь', 'у', 'нас', 'то']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [token for tweet in df.text for token in word_tokenize(tweet) if token not in punctuation]\n",
    "print(len(corpus))\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "_oWC7NpkLVSO",
    "outputId": "965b9fbd-6328-4c13-f20f-cd3714d1adb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('не', 69472),\n",
       " ('и', 55166),\n",
       " ('в', 52902),\n",
       " ('я', 52818),\n",
       " ('RT', 38070),\n",
       " ('на', 35759),\n",
       " ('http', 32998),\n",
       " ('что', 31541),\n",
       " ('с', 27217),\n",
       " ('а', 26860)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dict = Counter(corpus)\n",
    "freq_dict_sorted= sorted(freq_dict.items(), key=lambda x: -x[1])\n",
    "list(freq_dict_sorted)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "FrPkce0SLVSQ",
    "outputId": "d2ab5675-433a-480a-90ee-fa5dd9890922"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "first_100_freqs = [freq for word, freq in freq_dict_sorted[:100]]\n",
    "plt.plot(first_100_freqs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_N5V_K-LVSU"
   },
   "source": [
    "Закон Хипса -- обратная сторона закона Ципфа. Он описывает, что чем больше корпус, тем меньше новых слов добавляется с добавлением новых текстов. В какой-то момент корпус насыщается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw0GieJSMU-O"
   },
   "source": [
    "## Задание 1.\n",
    "\n",
    "**Задание**: обучите три классификатора: \n",
    "\n",
    "1) на токенах с высокой частотой \n",
    "\n",
    "2) на токенах со средней частотой \n",
    "\n",
    "3) на токенах с низкой частотой\n",
    "\n",
    "Сравните полученные результаты, оцените какие токены наиболее важные для классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('не', 69472),\n",
       " ('и', 55166),\n",
       " ('в', 52902),\n",
       " ('я', 52818),\n",
       " ('RT', 38070),\n",
       " ('на', 35759),\n",
       " ('http', 32998),\n",
       " ('что', 31541),\n",
       " ('с', 27217),\n",
       " ('а', 26860)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dict_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "high = set([i[0].lower() for i in freq_dict_sorted if i[1] > 5000 and i[0].lower() not in noise])\n",
    "medium = set([i[0].lower() for i in freq_dict_sorted if 300 <= i[1] < 5000 and i[0].lower() not in noise])\n",
    "low = set([i[0].lower() for i in freq_dict_sorted if i[1] < 300 and i[0].lower() not in noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 600, 315191)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(high), len(medium), len(low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_preproc_custom(text, frequency):\n",
    "    text = re.sub('[{}]'.format(punctuation), '', text)\n",
    "    text = mystem_analyzer.lemmatize(text)\n",
    "    return [word for word in text if word in frequency]\n",
    "\n",
    "# def my_preproc_medium(text):\n",
    "#     text = re.sub('[{}]'.format(punctuation), '', text)\n",
    "#     text = mystem_analyzer.lemmatize(text)\n",
    "#     return [word for word in text if word in high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=lambda x: my_preproc_custom(x, high))\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.53      0.64     41938\n",
      "    positive       0.32      0.62      0.42     14771\n",
      "\n",
      "    accuracy                           0.55     56709\n",
      "   macro avg       0.56      0.57      0.53     56709\n",
      "weighted avg       0.67      0.55      0.58     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.47      0.65      0.55     20256\n",
      "    positive       0.75      0.59      0.66     36453\n",
      "\n",
      "    accuracy                           0.61     56709\n",
      "   macro avg       0.61      0.62      0.60     56709\n",
      "weighted avg       0.65      0.61      0.62     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=lambda x: my_preproc_custom(x, medium))\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.72      0.74     29278\n",
      "    positive       0.72      0.75      0.73     27431\n",
      "\n",
      "    accuracy                           0.74     56709\n",
      "   macro avg       0.74      0.74      0.74     56709\n",
      "weighted avg       0.74      0.74      0.74     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=lambda x: my_preproc_custom(x, low))\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Логично, что для обобщающей способности классификатора наиболее важны редкие токены"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV3fmzp-LVSU"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## О важности эксплоративного анализа\n",
    "\n",
    "Но иногда пунктуация бывает и не шумом -- главное отталкиваться от задачи. Что будет если вообще не убирать пунктуацию?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "qjkMxK9VLVSV",
    "outputId": "dfea56d5-4d92-4862-9788-29c8c8db29ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00     27876\n",
      "    positive       1.00      1.00      1.00     28833\n",
      "\n",
      "    accuracy                           1.00     56709\n",
      "   macro avg       1.00      1.00      1.00     56709\n",
      "weighted avg       1.00      1.00      1.00     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize)\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2fRbUAvLVSX"
   },
   "source": [
    "Шок! Стоило оставить пунктуацию -- и все метрики равны 1. Как это получилось? Среди неё были очень значимые токены (как вы думаете, какие?). Найдите фичи с самыми большими коэффициэнтами:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2.\n",
    "\n",
    "найти фичи с наибольшей значимостью, и вывести их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(')', 58.28809624959113),\n",
       " ('d', 26.901393960038757),\n",
       " ('dd', 10.622200359635846),\n",
       " ('^_^', 9.176231553227357),\n",
       " ('ddd', 8.134235722014749),\n",
       " ('-d', 7.273158195647029),\n",
       " ('*', 7.238638002729514),\n",
       " (':', 5.976265660415659),\n",
       " ('dddd', 4.802664458674663),\n",
       " ('ddddd', 2.9528262983503946),\n",
       " ('dddddd', 1.819570767682661),\n",
       " ('=^_^=', 1.7299306783359976),\n",
       " ('люблю', 1.6699543603888973),\n",
       " ('спасибо', 1.602575559114406),\n",
       " ('х', 1.5205892917957158),\n",
       " ('%', 1.3675840094458107),\n",
       " ('рождения', 1.2564531990855863),\n",
       " ('ахахах', 1.2177909883208682),\n",
       " ('ахах', 1.1850680145402248),\n",
       " ('dddddddd', 1.1774892492611042)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(i, j) for i, j in zip(vec.get_feature_names(), clf.coef_[0])], key=lambda x: -x[1])[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vtAyItvLVSb"
   },
   "source": [
    "Посмотрим, как один из супер-значительных токенов справится с классификацией безо всякого машинного обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "uqH07o-7LVSc",
    "outputId": "fad0a24a-98ee-4f84-8782-495548eb0fb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92     32756\n",
      "    positive       0.83      1.00      0.91     23953\n",
      "\n",
      "    accuracy                           0.91     56709\n",
      "   macro avg       0.92      0.93      0.91     56709\n",
      "weighted avg       0.93      0.91      0.92     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cool_token = ')'\n",
    "pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5THCOjMLVSg"
   },
   "source": [
    "## Символьные n-граммы\n",
    "\n",
    "Теперь в качестве фичей используем, например, униграммы символов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "AIUwDOabLVSh",
    "outputId": "54f129b1-994f-448e-e861-1912b4a21cdc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      1.00      0.99     27830\n",
      "    positive       1.00      0.99      1.00     28879\n",
      "\n",
      "    accuracy                           0.99     56709\n",
      "   macro avg       0.99      1.00      0.99     56709\n",
      "weighted avg       0.99      0.99      0.99     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(analyzer='char', ngram_range=(1, 1))\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_E0uPpgLVSj"
   },
   "source": [
    "В общем-то, теперь уже понятно, почему на этих данных здесь 1. Так или инчае, на символах классифицировать тоже можно: для некторых задач (например, для определения языка) фичи-символьные n-граммы решительно рулят.\n",
    "\n",
    "Ещё одна замечательная особенность фичей-символов: токенизация и лемматизация не нужна, можно использовать такой подход для языков, у которых нет готвых анализаторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.\n",
    "\n",
    "1) сравнить count/tf-idf/hashing векторайзеры/полносвязанную сетку (построить classification_report)\n",
    "\n",
    "2) подобрать оптимальный размер для hashing векторайзера \n",
    "\n",
    "3) убедиться что для сетки нет переобучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=noise)\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.76      0.78     29662\n",
      "    positive       0.75      0.80      0.78     27047\n",
      "\n",
      "    accuracy                           0.78     56709\n",
      "   macro avg       0.78      0.78      0.78     56709\n",
      "weighted avg       0.78      0.78      0.78     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77     28081\n",
      "    positive       0.78      0.78      0.78     28628\n",
      "\n",
      "    accuracy                           0.78     56709\n",
      "   macro avg       0.78      0.78      0.78     56709\n",
      "weighted avg       0.78      0.78      0.78     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=noise)\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.65      0.67     29665\n",
      "    positive       0.64      0.68      0.66     27044\n",
      "\n",
      "    accuracy                           0.67     56709\n",
      "   macro avg       0.67      0.67      0.67     56709\n",
      "weighted avg       0.67      0.67      0.67     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vec = HashingVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=noise, n_features=2**10)\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.76      0.77     28504\n",
      "    positive       0.76      0.78      0.77     28205\n",
      "\n",
      "    accuracy                           0.77     56709\n",
      "   macro avg       0.77      0.77      0.77     56709\n",
      "weighted avg       0.77      0.77      0.77     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vec = HashingVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=noise, n_features=2**20)\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.76      0.77     28466\n",
      "    positive       0.76      0.78      0.77     28243\n",
      "\n",
      "    accuracy                           0.77     56709\n",
      "   macro avg       0.77      0.77      0.77     56709\n",
      "weighted avg       0.77      0.77      0.77     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vec = HashingVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=noise, n_features=2**25)\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимальный размер векторайзера = 2**20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<170125x1048576 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1392940 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170125, 1048576)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(bow.getrow(0).toarray(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hashing_dataset(Dataset):\n",
    "    def __init__(self, df, target):\n",
    "        self.df = df\n",
    "        self.target = target.replace({'positive': 1, 'negative': 0})\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.df.getrow(idx).toarray(), dtype=torch.float32), self.target.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = hashing_dataset(bow, y_train)\n",
    "test_df = hashing_dataset(bow, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_df, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_df, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_dim = 1048576\n",
    "        self.fc1 = nn.Linear(self.input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.l_relu = nn.LeakyReLU()\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.l_relu(x)\n",
    "        x = self.dp(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.l_relu(x)\n",
    "        x = self.dp(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "loss_func = nn.BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-145-947e1afd9a5b>:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(5)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff033bbc88e4a26853730cf4f2c9e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-947e1afd9a5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch-1.9.0-py3.8-macosx-10.9-x86_64.egg/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch-1.9.0-py3.8-macosx-10.9-x86_64.egg/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_WeightedLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_WeightedLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch-1.9.0-py3.8-macosx-10.9-x86_64.egg/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch-1.9.0-py3.8-macosx-10.9-x86_64.egg/torch/nn/_reduction.py\u001b[0m in \u001b[0;36mlegacy_get_string\u001b[0;34m(size_average, reduce, emit_warning)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mreduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm_notebook(range(5)):\n",
    "    for x_train, y_train in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x_train)\n",
    "        loss = loss_func(pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        for x_test, y_test in test_loader:\n",
    "            pred = model(x_test)\n",
    "            loss = loss_func(pred, y_test)\n",
    "            losses.append(loss)\n",
    "        print(f'optimizer {opt_name} epoch {epoch+1} test loss = {np.mean(losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5401]],\n",
       "\n",
       "        [[0.5426]],\n",
       "\n",
       "        [[0.5415]],\n",
       "\n",
       "        [[0.5415]],\n",
       "\n",
       "        [[0.5405]],\n",
       "\n",
       "        [[0.5425]],\n",
       "\n",
       "        [[0.5413]],\n",
       "\n",
       "        [[0.5403]],\n",
       "\n",
       "        [[0.5432]],\n",
       "\n",
       "        [[0.5453]],\n",
       "\n",
       "        [[0.5386]],\n",
       "\n",
       "        [[0.5382]],\n",
       "\n",
       "        [[0.5403]],\n",
       "\n",
       "        [[0.5449]],\n",
       "\n",
       "        [[0.5408]],\n",
       "\n",
       "        [[0.5391]],\n",
       "\n",
       "        [[0.5429]],\n",
       "\n",
       "        [[0.5461]],\n",
       "\n",
       "        [[0.5387]],\n",
       "\n",
       "        [[0.5400]],\n",
       "\n",
       "        [[0.5416]],\n",
       "\n",
       "        [[0.5440]],\n",
       "\n",
       "        [[0.5409]],\n",
       "\n",
       "        [[0.5384]],\n",
       "\n",
       "        [[0.5381]],\n",
       "\n",
       "        [[0.5438]],\n",
       "\n",
       "        [[0.5383]],\n",
       "\n",
       "        [[0.5427]],\n",
       "\n",
       "        [[0.5403]],\n",
       "\n",
       "        [[0.5424]],\n",
       "\n",
       "        [[0.5396]],\n",
       "\n",
       "        [[0.5375]],\n",
       "\n",
       "        [[0.5408]],\n",
       "\n",
       "        [[0.5423]],\n",
       "\n",
       "        [[0.5405]],\n",
       "\n",
       "        [[0.5405]],\n",
       "\n",
       "        [[0.5423]],\n",
       "\n",
       "        [[0.5398]],\n",
       "\n",
       "        [[0.5427]],\n",
       "\n",
       "        [[0.5379]],\n",
       "\n",
       "        [[0.5386]],\n",
       "\n",
       "        [[0.5397]],\n",
       "\n",
       "        [[0.5406]],\n",
       "\n",
       "        [[0.5411]],\n",
       "\n",
       "        [[0.5397]],\n",
       "\n",
       "        [[0.5386]],\n",
       "\n",
       "        [[0.5381]],\n",
       "\n",
       "        [[0.5437]],\n",
       "\n",
       "        [[0.5351]],\n",
       "\n",
       "        [[0.5383]],\n",
       "\n",
       "        [[0.5414]],\n",
       "\n",
       "        [[0.5404]],\n",
       "\n",
       "        [[0.5396]],\n",
       "\n",
       "        [[0.5381]],\n",
       "\n",
       "        [[0.5379]],\n",
       "\n",
       "        [[0.5386]],\n",
       "\n",
       "        [[0.5370]],\n",
       "\n",
       "        [[0.5405]],\n",
       "\n",
       "        [[0.5409]],\n",
       "\n",
       "        [[0.5440]],\n",
       "\n",
       "        [[0.5422]],\n",
       "\n",
       "        [[0.5388]],\n",
       "\n",
       "        [[0.5417]],\n",
       "\n",
       "        [[0.5431]]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gJABxhalLVQu",
    "IaQMCGHFLVQ6",
    "5AJk1B39LVRP",
    "RJlvqWuALVRs",
    "rck5OVqhLVSA",
    "mV3fmzp-LVSU",
    "H5THCOjMLVSg",
    "02s2Vh7MLVSj",
    "b1khxRFDLVSm",
    "sfUmWcAQLVSt",
    "BxvtN-3zLVS5",
    "gyrHhYkgLVTB"
   ],
   "name": "sem1_intro_common.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
